
import os
from openai import OpenAI # type: ignore
from dotenv import load_dotenv
from compiler import generate
import json

def stt(token, filename, background_video_name):
    client = OpenAI(api_key=token, base_url="https://api.groq.com/openai/v1") # connecting to Groq
    with open(filename, "rb") as file:
        transcription = client.audio.transcriptions.create(
        file=(filename, file.read()),
        model="whisper-large-v3",
        response_format="verbose_json",
        timestamp_granularities=["word"],
        ) # Making use of whisper by OpenAI and storing the timestamps in a variable called transcription
    ts = os.path.join(os.path.dirname(__file__), "transcript.txt") # Going to the path of transcript.txt
    with open(ts, "r") as f: # We compare the words generated by whisper and the actual words in transcript.txt, and modify it accordingly in case of a discrepancy
        text = f.read()
        words = text.split()
        if len(words) == len(transcription.words):
            for i, word in enumerate(transcription.words):
                if words[i] != word.word:
                    word.word = words[i]
    
    transcription_data = [
        {
            "word": word.word,
            "start": word.start,
            "end": word.end
        }
        for word in transcription.words
    ]      


    with open("transcription.json", "w", encoding="utf-8") as f:
        json.dump(transcription_data, f, ensure_ascii=False, indent=2) # Dumping transcription_data into a json file

    output_path = generate(transcription, filename, background_video_name)   # The final step, generating the videoo
    return output_path
    


if __name__ == "__main__": # Random testcase incase this file is executed as main.
    load_dotenv()
    
    token = os.getenv("LLM_API_TOKEN")
    if not token:
        print("Could not find token in the environment variables")
    else:
        stt(token, "/Users/cloudstone/Reelapse/generator/output_with_subtitles_f8483305b6a04bb48a6c04e7bb75def2.wav")
    